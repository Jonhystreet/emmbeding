{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Id  \\\n",
      "0  CONFLICTO COMPETENCIAL 14/2022   \n",
      "1  CONFLICTO COMPETENCIAL 14/2022   \n",
      "2  CONFLICTO COMPETENCIAL 14/2022   \n",
      "3  CONFLICTO COMPETENCIAL 13/2022   \n",
      "4  CONFLICTO COMPETENCIAL 13/2022   \n",
      "\n",
      "                                         Informacion     Etiqueta  \n",
      "0  Solicitante: EL PRIMER TRIBUNAL COLEGIADO EN M...  Solicitante  \n",
      "1            PONENTE: MINISTRA YASMÍN ESQUIVEL MOSSA      Ponente  \n",
      "2                  SECRETARIO: FANUEL MARTÍNEZ LÓPEZ   Secretario  \n",
      "3  SUSCITADO ENTRE EL TRIBUNAL COLEGIADO EN MATER...  Solicitante  \n",
      "4            PONENTE: MINISTRO JAVIER LAYNEZ POTISEK      Ponente  \n",
      "\n",
      " \n",
      " ------------------------------------------------------------------------------- \n",
      " \n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:No sentence-transformers model found with name D:\\Usuarios\\UGACJSS3/.cache\\torch\\sentence_transformers\\scjnugacj_jurisbert. Creating a new one with MEAN pooling.\n",
      "Some weights of the model checkpoint at D:\\Usuarios\\UGACJSS3/.cache\\torch\\sentence_transformers\\scjnugacj_jurisbert were not used when initializing RobertaModel: ['lm_head.layer_norm.bias', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.bias', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at D:\\Usuarios\\UGACJSS3/.cache\\torch\\sentence_transformers\\scjnugacj_jurisbert and are newly initialized: ['roberta.pooler.dense.bias', 'roberta.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tamaño del vector de embeddings:  637\n",
      "637\n",
      "<annoy.Annoy object at 0x0000021D146FA7F0>\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "Vector has wrong length (expected 637, got 768)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32md:\\Usuarios\\UGACJSS3\\Desktop\\emmbeding\\annoy\\Annoy.ipynb Cell 1'\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Usuarios/UGACJSS3/Desktop/emmbeding/annoy/Annoy.ipynb#ch0000000?line=28'>29</a>\u001b[0m     v \u001b[39m=\u001b[39m sentence_embeddings[i]\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Usuarios/UGACJSS3/Desktop/emmbeding/annoy/Annoy.ipynb#ch0000000?line=29'>30</a>\u001b[0m     \u001b[39m#En t.add_item debemos especificar el tamaño del vector que le damos es decir el primer parametro son la cantidad de datos que le daremos\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Usuarios/UGACJSS3/Desktop/emmbeding/annoy/Annoy.ipynb#ch0000000?line=30'>31</a>\u001b[0m     \u001b[39m#El segundo paarametro los vectores que embedeamos\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/d%3A/Usuarios/UGACJSS3/Desktop/emmbeding/annoy/Annoy.ipynb#ch0000000?line=31'>32</a>\u001b[0m     t\u001b[39m.\u001b[39;49madd_item(i, v)\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Usuarios/UGACJSS3/Desktop/emmbeding/annoy/Annoy.ipynb#ch0000000?line=33'>34</a>\u001b[0m t\u001b[39m.\u001b[39mbuild(\u001b[39m10\u001b[39m)\u001b[39m#Trabajamos los arboles y el segundo parametro es la cantiudad de nucleos que va utilizar\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/d%3A/Usuarios/UGACJSS3/Desktop/emmbeding/annoy/Annoy.ipynb#ch0000000?line=34'>35</a>\u001b[0m t\u001b[39m.\u001b[39msave(\u001b[39m'\u001b[39m\u001b[39mtess.ann\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: Vector has wrong length (expected 637, got 768)"
     ]
    }
   ],
   "source": [
    "from annoy import AnnoyIndex\n",
    "import pandas as pd\n",
    "import random\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from io import StringIO\n",
    "\n",
    "i = 0\n",
    "texto = \"\" \n",
    "#Importar e inizializar la informacion  \n",
    "info = 'D:\\\\Usuarios\\\\UGACJSS3\\\\Desktop\\\\emmbeding\\\\Muestra200_data.csv'\n",
    "data = pd.read_csv(info, sep=',',encoding='latin-1')\n",
    "print(data.head())\n",
    "print('\\n \\n ------------------------------------------------------------------------------- \\n \\n')\n",
    "#Tomar las columna de informacion\n",
    "sentences = data['Informacion'].tolist()\n",
    "#Encodear\n",
    "model = SentenceTransformer('scjnugacj/jurisbert')\n",
    "sentence_embeddings = model.encode(sentences)\n",
    "distance=len(sentence_embeddings)\n",
    "print(\"tamaño del vector de embeddings: \",distance)\n",
    "\n",
    "f = len(sentence_embeddings)\n",
    "print(f)\n",
    "#En el annoyindex la primera variable sera de la longitud de los vectores es decir en este caso seran de 768\n",
    "t = AnnoyIndex(f, 'angular')\n",
    "print(t)\n",
    "\n",
    "for i in range(0,len(sentences)):\n",
    "    v = sentence_embeddings[i]\n",
    "    #En t.add_item debemos especificar el tamaño del vector que le damos es decir el primer parametro son la cantidad de datos que le daremos\n",
    "    #El segundo paarametro los vectores que embedeamos\n",
    "    t.add_item(i, v)\n",
    "\n",
    "t.build(10)#Trabajamos los arboles y el segundo parametro es la cantiudad de nucleos que va utilizar\n",
    "t.save('tess.ann')\n",
    "u = AnnoyIndex(f,'angular')\n",
    "#t.load('test.ann')#Mapeado de el archivo\n",
    "print(t.get_nss_by_item(0,100))\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "print(f)\n",
    "\n",
    "for i in range(1000):\n",
    "    v = [random.gauss(0, 1) for z in range(f)]\n",
    "    t.add_item(i, v)\n",
    "\n",
    "t.build(10) # 10 trees\n",
    "t.save('test.ann')\n",
    "\n",
    "u = AnnoyIndex(f, 'angular')\n",
    "u.load('test.ann') # super fast, will just mmap the file\n",
    "print(u.get_nns_by_item(0, 100)) # will find the 1000 nearest neighbors'''"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "7ba1bc94ccd02cfa060379d503179e1702b1042fb88ca4d15c56a82d1111cb83"
  },
  "kernelspec": {
   "display_name": "Python 3.10.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
